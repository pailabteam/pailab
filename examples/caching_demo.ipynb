{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching function results\n",
    "This notebook demonstrates how one can use the MLRepo to cache function calls. Especially when working interactively using jupyter notebooks it may be usefull to \n",
    "cache function values for time consuming functions. In this notebook we demonstrate how to use pailab's ml_cache decorator to realize caching for functions.\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pailab.ml_repo.repo import MLRepo, MLObjectType\n",
    "from pailab.ml_repo.repo_objects import RawData, RepoInfoKey, RepoObject\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_repo = MLRepo(user='job_runner_user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration we use a simple example: We just create two RawData objects, one holding input data of a function evaluated at random points and one artificially created model output (where we just add a andom number to the function values to simulate some kind of model output with error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(10000, 3)\n",
    "y = x[:,1]*x[:,2]+ x[:,0]\n",
    "y_approx =y + x[:,1]*np.random.rand(10000)\n",
    "data = RawData(x, ['x0', 'x1', 'x2'], y, ['f'], repo_info={RepoInfoKey.NAME: 'eval', RepoInfoKey.CATEGORY: MLObjectType.TRAINING_DATA})\n",
    "data_eval = RawData(y_approx, ['eval'],  repo_info={RepoInfoKey.NAME: 'error', RepoInfoKey.CATEGORY: MLObjectType.EVAL_DATA})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': '9ecab500-8938-11e9-a354-fc084a6691eb',\n",
       " 'eval': '9ecab500-8938-11e9-a354-fc084a6691eb'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_repo.add([data, data_eval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ml_cache decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple function computing the pointwise distance from the model function values to the input function values. To show what can be cached the function returns  a string, a double value, a numpy array and the numpy array encapsulated in a RawData object.\n",
    "A print statement shows if the function has been executed. We use the ml_cache decorator to avoid evaluations if inputs have not been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pailab.tools.tools import ml_cache\n",
    "\n",
    "@ml_cache\n",
    "def eval_error(input_data, eval_data, factor):\n",
    "    error = input_data.y_data-eval_data.x_data\n",
    "    error_repo = RawData(error, ['error'], repo_info={RepoInfoKey.NAME: 'error', RepoInfoKey.CATEGORY: MLObjectType.CACHED_VALUE})\n",
    "    error_mean = error.mean()\n",
    "    print('Has been evaluated!')\n",
    "    return 'super', factor*error_mean, y, error_repo, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the function for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has been evaluated!\n",
      "('super', -0.2486461043058773, array([ 0.81820752,  0.77443111,  0.5684536 , ...,  0.98488201,\n",
      "        0.69648368,  1.21471749]), <pailab.ml_repo.repo_objects.RawData object at 0x000001DBAADA8828>, array([[-0.20371919],\n",
      "       [-0.31185704],\n",
      "       [-0.2126691 ],\n",
      "       ..., \n",
      "       [-0.2991671 ],\n",
      "       [-0.28365248],\n",
      "       [-0.22540838]]))\n"
     ]
    }
   ],
   "source": [
    "results = eval_error(ml_repo, data, data_eval, 1.0)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call the function again using the same arguments, the function is not executed, but results are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('super', -0.2486461043058773, array([ 0.81820752,  0.77443111,  0.5684536 , ...,  0.98488201,\n",
      "        0.69648368,  1.21471749]), <pailab.ml_repo.repo_objects.RawData object at 0x000001DBAADA8400>, array([[-0.20371919],\n",
      "       [-0.31185704],\n",
      "       [-0.2126691 ],\n",
      "       ..., \n",
      "       [-0.2991671 ],\n",
      "       [-0.28365248],\n",
      "       [-0.22540838]]))\n"
     ]
    }
   ],
   "source": [
    "results = eval_error(ml_repo, data, data_eval, 1.0)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the function with a modified argument which leads to a new function evaluatio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has been evaluated!\n",
      "('super', -0.4972922086117546, array([ 0.81820752,  0.77443111,  0.5684536 , ...,  0.98488201,\n",
      "        0.69648368,  1.21471749]), <pailab.ml_repo.repo_objects.RawData object at 0x000001DBAADA8898>, array([[-0.20371919],\n",
      "       [-0.31185704],\n",
      "       [-0.2126691 ],\n",
      "       ..., \n",
      "       [-0.2991671 ],\n",
      "       [-0.28365248],\n",
      "       [-0.22540838]]))\n"
     ]
    }
   ],
   "source": [
    "results = eval_error(ml_repo, data, data_eval, 2.0)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous argument returns the previous result without evaluating the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('super', -0.2486461043058773, array([ 0.81820752,  0.77443111,  0.5684536 , ...,  0.98488201,\n",
      "        0.69648368,  1.21471749]), <pailab.ml_repo.repo_objects.RawData object at 0x000001DBAADA89B0>, array([[-0.20371919],\n",
      "       [-0.31185704],\n",
      "       [-0.2126691 ],\n",
      "       ..., \n",
      "       [-0.2991671 ],\n",
      "       [-0.28365248],\n",
      "       [-0.22540838]]))\n"
     ]
    }
   ],
   "source": [
    "results = eval_error(ml_repo, data, data_eval, 1.0)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pailab.tools.tree import MLTree\n",
    "MLTree.add_tree(ml_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_error': [{'RepoInfoKey.AUTHOR': 'job_runner_user',\n",
       "   'RepoInfoKey.COMMIT_DATE': '2019-06-07 17:26:29.969607',\n",
       "   'RepoInfoKey.COMMIT_MESSAGE': '',\n",
       "   'RepoInfoKey.NAME': 'eval_error'},\n",
       "  {'RepoInfoKey.AUTHOR': 'job_runner_user',\n",
       "   'RepoInfoKey.COMMIT_DATE': '2019-06-07 17:26:30.081459',\n",
       "   'RepoInfoKey.COMMIT_MESSAGE': '',\n",
       "   'RepoInfoKey.NAME': 'eval_error'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_repo.tree.cache.eval_error.history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "786px",
    "left": "0px",
    "right": "1470.45px",
    "top": "65.9943px",
    "width": "260px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
